# ğŸ§ª `scraper.py` Usage Guide

The `scraper.py` CLI tool is used to check for updates on government regulatory websites for all 50 U.S. states, supporting flexible filters, exports, and change detection.

---

## ğŸš€ Basic Usage

```bash
python3 app/scraper.py
````

Scrapes **all URLs across all states** defined in `config/state_urls.json`.

---

## ğŸ›ï¸ Run by State

```bash
python3 app/scraper.py --state Texas
```

Checks all sites defined for the state of Texas.

---

## ğŸŒ Run for Specific URL

```bash
python3 app/scraper.py --state Texas --url https://example.com/regulation
```

Checks a **specific URL** and associates it with the state (used for output paths).

---

## ğŸ“ Export Formats

You can export results in the following formats:

### JSON (default)

```bash
python3 app/scraper.py --state Alaska --export json
```

### Markdown

```bash
python3 app/scraper.py --state Alaska --export markdown
```

### CSV

```bash
python3 app/scraper.py --state Alaska --export csv
```

---

## â™»ï¸ Only Updated Results

```bash
python3 app/scraper.py --state California --only-updated --export json
```

Skips writing or reporting unchanged pages.

---

## ğŸ·ï¸ Filter by Tags

Use `--filter-tag` to limit the scrape to URLs with **specific tags**.

```bash
python3 app/scraper.py --filter-tag air
python3 app/scraper.py --filter-tag air,permits
```

---

## âŒ Exclude Tags

Skip URLs based on tags using `--exclude-tag`.

```bash
python3 app/scraper.py --exclude-tag water
python3 app/scraper.py --exclude-tag air,permits
```

---

## ğŸ”€ Combine Filters

Example: Check all `Texas` URLs with `oil` or `gas`, but **not** `permits`:

```bash
python3 app/scraper.py --state Texas --filter-tag oil,gas --exclude-tag permits
```

---

## ğŸ“¦ Output Directory Structure

Results are saved into:

```
results/
â””â”€â”€ state/
    â””â”€â”€ <StateName>/
        â”œâ”€â”€ json/
        â”œâ”€â”€ csv/
        â”œâ”€â”€ markdown/
        â”œâ”€â”€ diff/
        â””â”€â”€ content/
```

Each file includes a timestamp and slug from the URL:

```
results/state/Texas/json/example_com_regulation_2025-08-17T18-55-00Z.json
```

---

## ğŸ§  Summary of CLI Options

| Option           | Description                                         |
| ---------------- | --------------------------------------------------- |
| `--state`        | Filter by state name (e.g. `Texas`)                 |
| `--url`          | Check a specific URL for changes                    |
| `--export`       | Export format: `json`, `csv`, or `markdown`         |
| `--only-updated` | Only export changed pages                           |
| `--filter-tag`   | Include only URLs with these tags (comma-separated) |
| `--exclude-tag`  | Exclude URLs with these tags (comma-separated)      |

---

## ğŸ§¼ Caching

* Hashes of previously scraped content are saved to `.cache/`
* Used to compare changes across runs
* Mapping of hash â URL/state is stored in `mapping.json`

---

## ğŸ“¥ Requirements

```bash
pip install -r requirements.txt
```

---

## ğŸ§ª Example: Run All & Export Only Air Permits

```bash
python3 app/scraper.py --filter-tag air,permits --only-updated --export markdown
```

---

## ğŸ‘¨â€ğŸ’» Contribute or Extend

Feel free to edit `scraper.py` or open an issue/PR to suggest new features!

