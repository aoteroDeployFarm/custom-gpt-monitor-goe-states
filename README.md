# ğŸ›¢ï¸ Regulatory Monitor Scraper

A modular Python project that tracks and monitors regulatory data across oil, gas, and energy websites in all 50 U.S. states. It supports both **command-line** and **API access**, and is designed to integrate with **OpenAI Custom GPTs** via Actions (OpenAPI 3.1.0).

---

## ğŸš€ Features

- ğŸ” **Scrape** hundreds of government regulatory URLs by state
- ğŸ§  **Detect changes** via hash-based comparison
- ğŸ§¾ **Export results** in JSON, CSV, or Markdown format
- ğŸ§ª **Run manually**, by cron, or as an API server
- ğŸ¤– **Integrate with GPTs** via OpenAPI-defined action

---

## ğŸ—‚ï¸ Project Directory Structure
---

## âš™ï¸ Installation

1. **Clone the repo**
   ```bash
   git clone https://github.com/your-username/regulatory-monitor.git
   
   cd regulatory-monitor
   
   python3 -m venv venv
   source venv/bin/activate
   
   pip install -r requirements.txt

## ğŸ§ª Usage
    Run the scraper via command line:
    
    python app/scraper.py                   # Check all states
    python app/scraper.py --state Texas     # Check one state
    python app/scraper.py --url https://... --state Texas
    python app/scraper.py --format csv      # Output CSV
    python app/scraper.py --format md       # Output Markdown

## ğŸŒ API Mode

    Run as a FastAPI service:
    uvicorn app.api:app --reload --port 8000
    GET /api/check
    GET /api/check?state=Texas
    GET /api/check?url=https://...

## ğŸ¤– GPT Integration (via Actions)
    1. Deploy the API using Render, Railway, Fly.io, etc.
    2. Register the OpenAPI spec in openapi/openapi.yaml via the GPT Builder.
    3. Prompt the GPT like:
       1. "Check for updates in Alaska's oil and gas regulations."

## ğŸ“„ Output Format
    Each record looks like:
        {
          "state": "Alaska",
          "url": "https://www.commerce.alaska.gov/web/aogcc/",
          "updated": true,
          "checkedAt": "2025-08-17T18:30:00Z",
          "summary": "Change detected"
        }

## ğŸ“… Automate It (Optional)
    To run daily with cron:

    crontab -e
    # Run at 2am daily
    0 2 * * * /path/to/venv/bin/python /path/to/regulatory-monitor/app/scraper.py

## ğŸ§¼ Caching
    * All previously scraped content is hashed (SHA256) and stored in .cache/
    * Prevents false positives on unchanged pages
    * URL content hashes are stored per URL in .cache/
    * This prevents false positives and unnecessary rescanning

## ğŸ› ï¸ Roadmap
    * Slack/email alert integration
    * HTML/text diff support
    * Docker support for easier deployment
    * Database to track long-term change history
    * Frontend dashboard for monitoring

## ğŸ³ (Optional) Docker Usage
    Coming soon! A Dockerfile can be added to simplify deployments. Ask if you'd like it now.

## ğŸ‘¨â€ğŸ’» Contributing
    Pull requests welcome! Fork the repo, create a branch, and submit a PR.

## ğŸ“„ License
    MIT License Â© Fabing Productions

## ğŸ™‹ Questions?
   * Open an issue
   * Start a discussion    
## ğŸ“ Project Directory Structure

```
custom-gpt-monitor-goe-state/
â”œâ”€â”€ app
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ api.py
â”‚Â Â  â””â”€â”€ scraper.py
â”œâ”€â”€ config
â”‚Â Â  â””â”€â”€ state_urls.json
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ init_project.sh
â”œâ”€â”€ NOTES.md
â”œâ”€â”€ openapi
â”‚Â Â  â””â”€â”€ openapi.yaml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ results
â”‚Â Â  â”œâ”€â”€ last_run.csv
â”‚Â Â  â”œâ”€â”€ last_run.json
â”‚Â Â  â””â”€â”€ last_run.md
â”œâ”€â”€ scripts
â”‚Â Â  â””â”€â”€ setup.sh
â””â”€â”€ setup.sh

6 directories, 15 files
```
