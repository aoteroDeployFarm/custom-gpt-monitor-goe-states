## ğŸ“ NOTES.md â€“ Project Summary & Quick Recovery Guide

---

### ğŸ›¢ï¸ Regulatory Monitor Scraper â€“ Project Overview

This project tracks and monitors regulatory websites across **all 50 U.S. states** related to the **oil, gas, and energy** sectors. It detects content updates, outputs change logs in multiple formats, and integrates with **OpenAI Custom GPTs** through a web API defined by an OpenAPI spec.

---

### âœ… Core Features

* Scrapes and monitors hundreds of URLs grouped by U.S. state
* Uses SHA-256 hashes to detect changes
* CLI interface for one-time or scheduled runs
* FastAPI-based web API for live checking or GPT integration
* Exports output in **JSON**, **CSV**, and **Markdown**
* Fully containerized using Docker
* Designed for cloud deployment or local use

---

### ğŸ§  Custom GPT Integration

* Exposes `/api/check` endpoint for GPT to query update status
* OpenAPI 3.1.0 spec available in `openapi/openapi.yaml`
* Deployable to services like **Render**, **Railway**, **Fly.io**, or any VPS
* GPTs can prompt with:

  > â€œCheck for updates in Alaskaâ€™s oil and gas regulatory data.â€

---

## ğŸ—‚ï¸ Project Directory Structure

regulatory-monitor/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ scraper.py       # Main CLI scraper logic
â”‚   â””â”€â”€ api.py           # FastAPI web server
â”œâ”€â”€ config/
â”‚   â””â”€â”€ state_urls.json  # URL list grouped by state
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ last_run.json
â”‚   â”œâ”€â”€ last_run.csv
â”‚   â””â”€â”€ last_run.md
â”œâ”€â”€ .cache/              # Local hash cache per URL
â”œâ”€â”€ openapi/
â”‚   â””â”€â”€ openapi.yaml     # Action spec for GPTs
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup.sh         # Directory bootstrap script
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

---

### ğŸ§ª How to Run

#### CLI Mode

```bash
python app/scraper.py                     # Run all states
python app/scraper.py --state Texas       # Run one state
python app/scraper.py --url https://...   # Run one URL
python app/scraper.py --format csv        # Output as CSV
```

#### API Mode (FastAPI)

```bash
uvicorn app.api:app --host 0.0.0.0 --port 8000
```

Example endpoints:

* `GET /api/check`
* `GET /api/check?state=Texas`
* `GET /api/check?url=https://example.com`

---

### ğŸ³ Docker Usage

#### Build the image

```bash
docker build -t regulatory-monitor .
```

#### Run the container

```bash
docker run -p 8000:8000 regulatory-monitor
```

Access the API:

```
http://localhost:8000/api/check
```

---

### âš™ï¸ Automation & Cron

To run daily (e.g., 2am):

```bash
crontab -e
# Add this line:
0 2 * * * /path/to/venv/bin/python /path/to/regulatory-monitor/app/scraper.py
```

---

### ğŸ“Œ Remaining TODOs / Future Features

* [ ] Add email or Slack alerts for updated URLs
* [ ] Implement content diffing (before vs after)
* [ ] Store historical change logs (file or DB)
* [ ] Add a frontend dashboard for browsing changes
* [ ] Add GitHub Actions for CI/CD (optional)
* [ ] Finalize and test `openapi/openapi.yaml`

---

### âœ… Current Status

* âœ… CLI scraper works
* âœ… API works locally and in Docker
* âœ… JSON config of all 50 states complete
* âœ… GPT integration via OpenAPI is prepared
* âœ… Project is Dockerized and deployable

---

### ğŸ“¦ Hosting Recommendations

* **Render** (easy Docker deploys)
* **Railway** (easy GitHub + Docker pipeline)
* **Fly.io** (if you want global deployments)
* Any VPS (with Docker installed)

---

### ğŸ™‹â€â™€ï¸ Help / Recovery

* Clone repo and run: `docker build` + `docker run`
* Use `NOTES.md` to restore structure and commands
* Use `setup.sh` in `scripts/` to recreate folders
* For GPT integration, deploy `/api/check` and register `openapi.yaml`

---

Save this file and youâ€™ll never lose context â€” even if your tab or session disappears. ğŸš€

---

Let me know if you'd like me to generate the missing pieces:

* `api.py`
* `openapi.yaml`
* `Docker Compose`
* GitHub Actions workflows

I'm ready whenever you are.
